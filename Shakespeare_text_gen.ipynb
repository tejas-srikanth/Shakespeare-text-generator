{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare_text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYpPxXAYSEIGdvjnHnsGpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejas-srikanth/Shakespeare-text-generator/blob/master/Shakespeare_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUSYc6JUUnls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3JqncLXjh8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93bab7ae-c5eb-438c-ee76-c1dbceb2e141"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "use_gpu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvBTfX_hWAYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "88836b2c-ed85-4b25-f6db-f7e2c6fca9c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHyjEjU-XGYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = '/content/gdrive/My Drive/Colab Notebooks/NLP_with_pytorch/shakespeare.txt'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHMfMnqbXVN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(root, 'r', encoding=\"utf8\") as f:\n",
        "  all_text = f.read()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSj2W9GWXWr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4d0b2435-c063-41c7-99fd-547a8b7af8f9"
      },
      "source": [
        "all_text[:500]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bu\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96dCYeiBZZpc",
        "colab_type": "text"
      },
      "source": [
        "# Encode values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O818SrEqZjHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = set(all_text)\n",
        "decoder = dict(enumerate(all_characters))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q00Rte2aaZaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = dict((d,idx) for idx,d in decoder.items())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYcFXTC6a6wD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text = np.array([encoder[char] for char in all_text])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XbKCZgMbNR0",
        "colab_type": "text"
      },
      "source": [
        "# One Hot Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9CSEsLHbRJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoder(encoded_text, num_unique_chars):\n",
        "\n",
        "   one_hot = np.zeros((encoded_text.size, num_unique_chars))\n",
        "\n",
        "   one_hot.astype(np.float32)\n",
        "\n",
        "   one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
        "\n",
        "   one_hot = one_hot.reshape(*encoded_text.shape, num_unique_chars)\n",
        "\n",
        "   return one_hot"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sSHWjxXcYMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23dfdde6-ea5e-42d7-8709-e9ad0426d13c"
      },
      "source": [
        "print(one_hot_encoder(np.array([0]), 3))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxnMU8_VdL8-",
        "colab_type": "text"
      },
      "source": [
        "# Batch creator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anemCJ-HdRUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_batches(encoded_text, samp_batch_size, seq_len):\n",
        "  total_chars_batch = samp_batch_size * seq_len\n",
        "  num_total_batches = int(len(encoded_text)/total_chars_batch)\n",
        "  enc_txt = encoded_text[:num_total_batches*total_chars_batch]\n",
        "  enc_txt = enc_txt.reshape(samp_batch_size, -1)\n",
        "\n",
        "  for n in range(0, enc_txt.shape[1], seq_len):\n",
        "    x = enc_txt[:,n:n+seq_len]\n",
        "    y = np.zeros_like(x)\n",
        "\n",
        "    try:\n",
        "      y[:,:-1] = x[:,1:]\n",
        "      y[:, -1] = enc_txt[:,n+seq_len]\n",
        "    except:\n",
        "      y[:,:-1] = x[:,1:]\n",
        "      y[:, -1] = enc_txt[:,0]\n",
        "    \n",
        "    yield x,y\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvKy6FOBfHei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9a348fc9-2a75-4a3b-b9dd-2babd2ffd51f"
      },
      "source": [
        "arr = np.arange(30)\n",
        "next(create_batches(arr, 2, 5))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0,  1,  2,  3,  4],\n",
              "        [15, 16, 17, 18, 19]]), array([[ 1,  2,  3,  4,  5],\n",
              "        [16, 17, 18, 19, 20]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RJxo1owlHs3",
        "colab_type": "text"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtA-NIsOfO0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self, all_characters, num_hidden=256, num_layers=4, drop_prob=0.5, use_gpu=False ):\n",
        "\n",
        "    super().__init__()\n",
        "    self.num_hidden = num_hidden\n",
        "    self.num_layers = num_layers\n",
        "    self.drop_prob = 0.5\n",
        "    self.use_gpu = use_gpu\n",
        "\n",
        "    self.all_characters = all_characters\n",
        "    self.decoder = dict(enumerate(all_characters))\n",
        "    self.encoder = dict((data, idx) for idx,data in self.decoder.items())\n",
        "\n",
        "    self.lstm = nn.LSTM(len(all_characters), hidden_size=num_hidden, num_layers=num_layers, batch_first=True, dropout=0.5)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc_linear = nn.Linear(num_hidden, len(all_characters))\n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    lstm_out, hidden = self.lstm(x, hidden)\n",
        "    drop_out = self.dropout(lstm_out).contiguous().view(-1, self.num_hidden)\n",
        "    x_out = self.fc_linear(drop_out)\n",
        "\n",
        "    return x_out, hidden\n",
        "  \n",
        "  def hidden(self, batch_size):\n",
        "    if self.use_gpu:\n",
        "      hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
        "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
        "    else:\n",
        "      hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
        "                torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHynDRXXn2-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(all_characters, num_hidden=512, num_layers=3, drop_prob=0.5, use_gpu=use_gpu)\n",
        "if use_gpu:\n",
        "  model = model.cuda()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swigccgjrSa2",
        "colab_type": "text"
      },
      "source": [
        "# Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEUgg2J1rv33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_percentage = 0.9\n",
        "num_train = int(len(encoded_text) * train_percentage)\n",
        "train_set = encoded_text[:num_train]\n",
        "val_set = encoded_text[num_train:]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58pvGO5sXSk",
        "colab_type": "text"
      },
      "source": [
        "# Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4h5zo3psZ0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS3SWVBxwzJ1",
        "colab_type": "text"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbivsH0-x30C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "seq_len = 100\n",
        "batch_size=128\n",
        "num_unique=max(encoded_text)+1\n",
        "\n",
        "tracker=0"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkQ8Az8-yNQX",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slaxzv16yRKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e22f5158-ca3a-4e24-cf7a-57fe283a923b"
      },
      "source": [
        "model.train()\n",
        "\n",
        "for i in range(epochs):\n",
        "  hidden_state = model.hidden(batch_size)\n",
        "\n",
        "  for x, y in create_batches(train_set, batch_size, seq_len):\n",
        "    tracker += 1\n",
        "\n",
        "    x = one_hot_encoder(x, num_unique)\n",
        "\n",
        "    inputs = torch.from_numpy(x).float()\n",
        "    target = torch.from_numpy(y)\n",
        "\n",
        "    if use_gpu:\n",
        "      inputs = inputs.cuda()\n",
        "      target = target.cuda()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    hidden_state = tuple([state.data for state in hidden_state])\n",
        "    output, hidden_state = model.forward(inputs, hidden_state)\n",
        "    loss = criterion(output, target.view(batch_size*seq_len).long())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if tracker % 25 == 0:\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      val_losses = []\n",
        "\n",
        "      hidden_val = model.hidden(batch_size)\n",
        "\n",
        "      for x_val, y_val in create_batches(val_set, batch_size, seq_len):\n",
        "        x_val = one_hot_encoder(x_val, num_unique)\n",
        "\n",
        "        input_val = torch.from_numpy(x_val).float()\n",
        "        target_val = torch.from_numpy(y_val)\n",
        "\n",
        "        if use_gpu:\n",
        "          input_val = input_val.cuda()\n",
        "          target_val = target_val.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        hidden_val = tuple([state.data for state in hidden_val])\n",
        "\n",
        "        out_val, hidden_val = model(input_val, hidden_val)\n",
        "        val_loss = criterion(out_val, target_val.view(batch_size*seq_len).long())\n",
        "\n",
        "      val_losses.append(loss.item())\n",
        "      if tracker % 100 == 0:\n",
        "        print(f'EPOCH: {i+1}    STEP: {tracker}   LOSS: {loss.item()}')\n",
        "\n",
        "      model.train()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 1    STEP: 100   LOSS: 3.066499710083008\n",
            "EPOCH: 1    STEP: 200   LOSS: 2.617043972015381\n",
            "EPOCH: 1    STEP: 300   LOSS: 2.2426953315734863\n",
            "EPOCH: 1    STEP: 400   LOSS: 2.0865156650543213\n",
            "EPOCH: 2    STEP: 500   LOSS: 1.9633419513702393\n",
            "EPOCH: 2    STEP: 600   LOSS: 1.8567490577697754\n",
            "EPOCH: 2    STEP: 700   LOSS: 1.8165791034698486\n",
            "EPOCH: 3    STEP: 800   LOSS: 1.7943572998046875\n",
            "EPOCH: 3    STEP: 900   LOSS: 1.6966782808303833\n",
            "EPOCH: 3    STEP: 1000   LOSS: 1.6200647354125977\n",
            "EPOCH: 3    STEP: 1100   LOSS: 1.6624664068222046\n",
            "EPOCH: 4    STEP: 1200   LOSS: 1.607407569885254\n",
            "EPOCH: 4    STEP: 1300   LOSS: 1.603095531463623\n",
            "EPOCH: 4    STEP: 1400   LOSS: 1.520401120185852\n",
            "EPOCH: 4    STEP: 1500   LOSS: 1.4756263494491577\n",
            "EPOCH: 5    STEP: 1600   LOSS: 1.4703242778778076\n",
            "EPOCH: 5    STEP: 1700   LOSS: 1.4506334066390991\n",
            "EPOCH: 5    STEP: 1800   LOSS: 1.4619078636169434\n",
            "EPOCH: 5    STEP: 1900   LOSS: 1.4399521350860596\n",
            "EPOCH: 6    STEP: 2000   LOSS: 1.449833631515503\n",
            "EPOCH: 6    STEP: 2100   LOSS: 1.3929921388626099\n",
            "EPOCH: 6    STEP: 2200   LOSS: 1.4380663633346558\n",
            "EPOCH: 6    STEP: 2300   LOSS: 1.3814620971679688\n",
            "EPOCH: 7    STEP: 2400   LOSS: 1.4059371948242188\n",
            "EPOCH: 7    STEP: 2500   LOSS: 1.3783694505691528\n",
            "EPOCH: 7    STEP: 2600   LOSS: 1.350062370300293\n",
            "EPOCH: 8    STEP: 2700   LOSS: 1.4169566631317139\n",
            "EPOCH: 8    STEP: 2800   LOSS: 1.323179006576538\n",
            "EPOCH: 8    STEP: 2900   LOSS: 1.3473321199417114\n",
            "EPOCH: 8    STEP: 3000   LOSS: 1.357816219329834\n",
            "EPOCH: 9    STEP: 3100   LOSS: 1.3169262409210205\n",
            "EPOCH: 9    STEP: 3200   LOSS: 1.3387638330459595\n",
            "EPOCH: 9    STEP: 3300   LOSS: 1.3511474132537842\n",
            "EPOCH: 9    STEP: 3400   LOSS: 1.3375080823898315\n",
            "EPOCH: 10    STEP: 3500   LOSS: 1.282986044883728\n",
            "EPOCH: 10    STEP: 3600   LOSS: 1.2977062463760376\n",
            "EPOCH: 10    STEP: 3700   LOSS: 1.2966253757476807\n",
            "EPOCH: 10    STEP: 3800   LOSS: 1.2547861337661743\n",
            "EPOCH: 11    STEP: 3900   LOSS: 1.3046772480010986\n",
            "EPOCH: 11    STEP: 4000   LOSS: 1.2868252992630005\n",
            "EPOCH: 11    STEP: 4100   LOSS: 1.2912074327468872\n",
            "EPOCH: 11    STEP: 4200   LOSS: 1.2469192743301392\n",
            "EPOCH: 12    STEP: 4300   LOSS: 1.2733291387557983\n",
            "EPOCH: 12    STEP: 4400   LOSS: 1.247637152671814\n",
            "EPOCH: 12    STEP: 4500   LOSS: 1.2399563789367676\n",
            "EPOCH: 12    STEP: 4600   LOSS: 1.2367236614227295\n",
            "EPOCH: 13    STEP: 4700   LOSS: 1.2677232027053833\n",
            "EPOCH: 13    STEP: 4800   LOSS: 1.2808678150177002\n",
            "EPOCH: 13    STEP: 4900   LOSS: 1.300376296043396\n",
            "EPOCH: 14    STEP: 5000   LOSS: 1.2669332027435303\n",
            "EPOCH: 14    STEP: 5100   LOSS: 1.242583990097046\n",
            "EPOCH: 14    STEP: 5200   LOSS: 1.2150967121124268\n",
            "EPOCH: 14    STEP: 5300   LOSS: 1.2398844957351685\n",
            "EPOCH: 15    STEP: 5400   LOSS: 1.2208638191223145\n",
            "EPOCH: 15    STEP: 5500   LOSS: 1.2470855712890625\n",
            "EPOCH: 15    STEP: 5600   LOSS: 1.2052814960479736\n",
            "EPOCH: 15    STEP: 5700   LOSS: 1.2643808126449585\n",
            "EPOCH: 16    STEP: 5800   LOSS: 1.23600435256958\n",
            "EPOCH: 16    STEP: 5900   LOSS: 1.186334252357483\n",
            "EPOCH: 16    STEP: 6000   LOSS: 1.265648365020752\n",
            "EPOCH: 16    STEP: 6100   LOSS: 1.183284044265747\n",
            "EPOCH: 17    STEP: 6200   LOSS: 1.2269567251205444\n",
            "EPOCH: 17    STEP: 6300   LOSS: 1.176844596862793\n",
            "EPOCH: 17    STEP: 6400   LOSS: 1.2232017517089844\n",
            "EPOCH: 17    STEP: 6500   LOSS: 1.2214335203170776\n",
            "EPOCH: 18    STEP: 6600   LOSS: 1.215416431427002\n",
            "EPOCH: 18    STEP: 6700   LOSS: 1.2167938947677612\n",
            "EPOCH: 18    STEP: 6800   LOSS: 1.231666088104248\n",
            "EPOCH: 18    STEP: 6900   LOSS: 1.216324806213379\n",
            "EPOCH: 19    STEP: 7000   LOSS: 1.1804804801940918\n",
            "EPOCH: 19    STEP: 7100   LOSS: 1.2152026891708374\n",
            "EPOCH: 19    STEP: 7200   LOSS: 1.1999038457870483\n",
            "EPOCH: 20    STEP: 7300   LOSS: 1.1865066289901733\n",
            "EPOCH: 20    STEP: 7400   LOSS: 1.2098859548568726\n",
            "EPOCH: 20    STEP: 7500   LOSS: 1.2022420167922974\n",
            "EPOCH: 20    STEP: 7600   LOSS: 1.2301474809646606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUdm5LZRGBIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/NLP_with_pytorch/512Hidden3Layers.pt')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoybqoQHkmlB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61d14ca1-2eae-4717-d659-8b159512ff96"
      },
      "source": [
        "model = Model(all_characters, num_hidden=512, num_layers=3, drop_prob=0.5, use_gpu=use_gpu)\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/Colab Notebooks/NLP_with_pytorch/512Hidden3Layers.pt'))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcmM81wFdnas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_character(model, char, hidden=None, k=1):\n",
        "  encoded_text = model.encoder[char]\n",
        "  encoded_text = one_hot_encoder(np.array([[encoded_text]]), len(model.all_characters))\n",
        "  input = torch.from_numpy(encoded_text)\n",
        "\n",
        "  if use_gpu:\n",
        "    input = input.float().cuda()\n",
        "  \n",
        "  hidden = tuple([state.data for state in hidden])\n",
        "  out, hidden = model(input, hidden)\n",
        "  probs = F.log_softmax(out, dim=1).data\n",
        "\n",
        "  probs, indices = probs.topk(k)\n",
        "  probs = probs.numpy().flatten()\n",
        "  indices = indices.numpy().squeeze()\n",
        "  probs = probs/probs.sum()\n",
        "  chr = np.random.choice(indices, p=probs)\n",
        "  return model.decoder[chr], hidden"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvGnhr-OoBXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, size, seed='The', k=1):\n",
        "    \n",
        "    if model.use_gpu:\n",
        "      model.cuda()\n",
        "    else:\n",
        "      model.cpu()\n",
        "    model.eval()\n",
        "    output = [c for c in seed]\n",
        "\n",
        "    hidden = model.hidden(1)\n",
        "    for c in seed:\n",
        "      out, hidden = predict_next_char(model, c, hidden=hidden, k=k)\n",
        "    output.append(out)\n",
        "\n",
        "    for i in range(size):\n",
        "      out, hidden = predict_next_char(model, output[-1], hidden=hidden, k=k)\n",
        "      output.append(out)\n",
        "    return ''.join(output)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPWqfF9ztroi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shakey = generate_text(model, 500, seed=\"The \", k=3)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGs95_7nt8Ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "6347ee13-a4dc-4dbf-e56b-96720be68ec8"
      },
      "source": [
        "print(shakey)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The way at mine\n",
            "    this things. If this were-therefore I heard the state,\n",
            "    If you shall be the chamber, and I have seen his sound\n",
            "    To see you all a man of my subjocce.  \n",
            "  CASSIO. I will be so made than he. I will not speak\n",
            "    The story of this brother and the son,\n",
            "    And so we will be then, the subjects take till\n",
            "    What they shall see this son to me to see.\n",
            "  CAESAR. It shall be so more than this strange anger,\n",
            "    The senting of the ways of such a parloy\n",
            "    And stand their beauty too mu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXsQWar2yzAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8824752-ebb2-4daa-e0ec-85ab858f1a54"
      },
      "source": [
        "f = open('/content/gdrive/My Drive/Colab Notebooks/NLP_with_pytorch/Data/ai_play.txt', 'w', encoding=\"utf8\")\n",
        "f.write(shakey)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60IIJGrjgMcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}