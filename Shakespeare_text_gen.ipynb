{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare_text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPW0ZhI5ixBvh3fVy2ypd3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejas-srikanth/Shakespeare-text-generator/blob/master/Shakespeare_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUSYc6JUUnls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3JqncLXjh8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16c1b51c-fac6-49f9-b699-1bac465f0fc4"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "use_gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvBTfX_hWAYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "921b5258-1fb3-4e22-8551-fdcc6b0db75f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHyjEjU-XGYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = '/content/gdrive/My Drive/Colab Notebooks/NLP_with_pytorch/Data/shakespeare.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHMfMnqbXVN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(root, 'r', encoding=\"utf8\") as f:\n",
        "  all_text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSj2W9GWXWr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6b35be30-c31e-4483-a689-459dcf5a7b15"
      },
      "source": [
        "all_text[:500]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bu\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96dCYeiBZZpc",
        "colab_type": "text"
      },
      "source": [
        "# Encode values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O818SrEqZjHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = set(all_text)\n",
        "decoder = dict(enumerate(all_characters))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q00Rte2aaZaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bab03a35-3e36-478f-9dc5-00000dba97e7"
      },
      "source": [
        "encoder = dict((d,idx) for idx,d in decoder.items())\n",
        "encoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 82,\n",
              " ' ': 39,\n",
              " '!': 75,\n",
              " '\"': 12,\n",
              " '&': 11,\n",
              " \"'\": 55,\n",
              " '(': 61,\n",
              " ')': 72,\n",
              " ',': 67,\n",
              " '-': 0,\n",
              " '.': 83,\n",
              " '0': 22,\n",
              " '1': 27,\n",
              " '2': 63,\n",
              " '3': 71,\n",
              " '4': 6,\n",
              " '5': 51,\n",
              " '6': 34,\n",
              " '7': 59,\n",
              " '8': 2,\n",
              " '9': 68,\n",
              " ':': 21,\n",
              " ';': 25,\n",
              " '<': 38,\n",
              " '>': 18,\n",
              " '?': 14,\n",
              " 'A': 4,\n",
              " 'B': 77,\n",
              " 'C': 24,\n",
              " 'D': 42,\n",
              " 'E': 64,\n",
              " 'F': 60,\n",
              " 'G': 36,\n",
              " 'H': 35,\n",
              " 'I': 28,\n",
              " 'J': 48,\n",
              " 'K': 47,\n",
              " 'L': 49,\n",
              " 'M': 20,\n",
              " 'N': 56,\n",
              " 'O': 57,\n",
              " 'P': 45,\n",
              " 'Q': 62,\n",
              " 'R': 15,\n",
              " 'S': 37,\n",
              " 'T': 33,\n",
              " 'U': 81,\n",
              " 'V': 23,\n",
              " 'W': 78,\n",
              " 'X': 8,\n",
              " 'Y': 30,\n",
              " 'Z': 58,\n",
              " '[': 26,\n",
              " ']': 32,\n",
              " '_': 9,\n",
              " '`': 65,\n",
              " 'a': 69,\n",
              " 'b': 44,\n",
              " 'c': 31,\n",
              " 'd': 40,\n",
              " 'e': 17,\n",
              " 'f': 43,\n",
              " 'g': 16,\n",
              " 'h': 13,\n",
              " 'i': 5,\n",
              " 'j': 46,\n",
              " 'k': 54,\n",
              " 'l': 80,\n",
              " 'm': 73,\n",
              " 'n': 66,\n",
              " 'o': 70,\n",
              " 'p': 79,\n",
              " 'q': 1,\n",
              " 'r': 53,\n",
              " 's': 76,\n",
              " 't': 7,\n",
              " 'u': 3,\n",
              " 'v': 52,\n",
              " 'w': 74,\n",
              " 'x': 29,\n",
              " 'y': 10,\n",
              " 'z': 41,\n",
              " '|': 19,\n",
              " '}': 50}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYcFXTC6a6wD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text = np.array([encoder[char] for char in all_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XbKCZgMbNR0",
        "colab_type": "text"
      },
      "source": [
        "# One Hot Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9CSEsLHbRJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoder(encoded_text, num_unique_chars):\n",
        "\n",
        "   one_hot = np.zeros((encoded_text.size, num_unique_chars))\n",
        "\n",
        "   one_hot.astype(np.float32)\n",
        "\n",
        "   one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
        "\n",
        "   one_hot = one_hot.reshape(*encoded_text.shape, num_unique_chars)\n",
        "\n",
        "   return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sSHWjxXcYMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c046b98-89c6-46a8-b761-ff26ee86722c"
      },
      "source": [
        "print(one_hot_encoder(np.array([0]), 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxnMU8_VdL8-",
        "colab_type": "text"
      },
      "source": [
        "# Batch creator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anemCJ-HdRUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_batches(encoded_text, samp_batch_size, seq_len):\n",
        "  total_chars_batch = samp_batch_size * seq_len\n",
        "  num_total_batches = int(len(encoded_text)/total_chars_batch)\n",
        "  enc_txt = encoded_text[:num_total_batches*total_chars_batch]\n",
        "  enc_txt = enc_txt.reshape(samp_batch_size, -1)\n",
        "\n",
        "  for n in range(0, enc_txt.shape[1], seq_len):\n",
        "    x = enc_txt[:,n:n+seq_len]\n",
        "    y = np.zeros_like(x)\n",
        "\n",
        "    try:\n",
        "      y[:,:-1] = x[:,1:]\n",
        "      y[:, -1] = enc_txt[:,n+seq_len]\n",
        "    except:\n",
        "      y[:,:-1] = x[:,1:]\n",
        "      y[:, -1] = enc_txt[:,0]\n",
        "    \n",
        "    yield x,y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvKy6FOBfHei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "dc7a1aab-e9a3-424d-a5d7-00a0acb1661f"
      },
      "source": [
        "arr = np.arange(30)\n",
        "next(create_batches(arr, 2, 5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0,  1,  2,  3,  4],\n",
              "        [15, 16, 17, 18, 19]]), array([[ 1,  2,  3,  4,  5],\n",
              "        [16, 17, 18, 19, 20]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RJxo1owlHs3",
        "colab_type": "text"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtA-NIsOfO0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self, all_characters, num_hidden=256, num_layers=4, drop_prob=0.5, use_gpu=False ):\n",
        "\n",
        "    super().__init__()\n",
        "    self.num_hidden = num_hidden\n",
        "    self.num_layers = num_layers\n",
        "    self.drop_prob = 0.5\n",
        "    self.use_gpu = use_gpu\n",
        "\n",
        "    self.all_characters = all_characters\n",
        "    self.decoder = dict(enumerate(all_characters))\n",
        "    self.encoder = dict((data, idx) for idx,data in self.decoder.items())\n",
        "\n",
        "    self.lstm = nn.LSTM(len(all_characters), hidden_size=num_hidden, num_layers=num_layers, batch_first=True, dropout=0.5)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc_linear = nn.Linear(num_hidden, len(all_characters))\n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    lstm_out, hidden = self.lstm(x, hidden)\n",
        "    drop_out = self.dropout(lstm_out).contiguous().view(-1, self.num_hidden)\n",
        "    x_out = self.fc_linear(drop_out)\n",
        "\n",
        "    return x_out, hidden\n",
        "  \n",
        "  def hidden(self, batch_size):\n",
        "    if self.use_gpu:\n",
        "      hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
        "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
        "    else:\n",
        "      hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
        "                torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHynDRXXn2-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(all_characters, num_hidden=512, num_layers=3, drop_prob=0.5, use_gpu=use_gpu)\n",
        "if use_gpu:\n",
        "  model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swigccgjrSa2",
        "colab_type": "text"
      },
      "source": [
        "# Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEUgg2J1rv33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_percentage = 0.9\n",
        "num_train = int(len(encoded_text) * train_percentage)\n",
        "train_set = encoded_text[:num_train]\n",
        "val_set = encoded_text[num_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58pvGO5sXSk",
        "colab_type": "text"
      },
      "source": [
        "# Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4h5zo3psZ0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS3SWVBxwzJ1",
        "colab_type": "text"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbivsH0-x30C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "seq_len = 100\n",
        "batch_size=128\n",
        "num_unique=max(encoded_text)+1\n",
        "\n",
        "tracker=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkQ8Az8-yNQX",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slaxzv16yRKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2f6b048-29c8-4b22-bb7a-2b27c26b2019"
      },
      "source": [
        "model.train()\n",
        "\n",
        "for i in range(epochs):\n",
        "  hidden_state = model.hidden(batch_size)\n",
        "\n",
        "  for x, y in create_batches(train_set, batch_size, seq_len):\n",
        "    tracker += 1\n",
        "\n",
        "    x = one_hot_encoder(x, num_unique)\n",
        "\n",
        "    inputs = torch.from_numpy(x).float()\n",
        "    target = torch.from_numpy(y)\n",
        "\n",
        "    if use_gpu:\n",
        "      inputs = inputs.cuda()\n",
        "      target = target.cuda()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    hidden_state = tuple([state.data for state in hidden_state])\n",
        "    output, hidden_state = model.forward(inputs, hidden_state)\n",
        "    loss = criterion(output, target.view(batch_size*seq_len).long())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if tracker % 25 == 0:\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      val_losses = []\n",
        "\n",
        "      hidden_val = model.hidden(batch_size)\n",
        "\n",
        "      for x_val, y_val in create_batches(val_set, batch_size, seq_len):\n",
        "        x_val = one_hot_encoder(x_val, num_unique)\n",
        "\n",
        "        input_val = torch.from_numpy(x_val).float()\n",
        "        target_val = torch.from_numpy(y_val)\n",
        "\n",
        "        if use_gpu:\n",
        "          input_val = input_val.cuda()\n",
        "          target_val = target_val.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        hidden_val = tuple([state.data for state in hidden_val])\n",
        "\n",
        "        out_val, hidden_val = model(input_val, hidden_val)\n",
        "        val_loss = criterion(out_val, target_val.view(batch_size*seq_len).long())\n",
        "\n",
        "      val_losses.append(loss.item())\n",
        "\n",
        "      print(f'EPOCH: {i+1}    STEP: {tracker}   LOSS: {loss.item()}')\n",
        "\n",
        "      model.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 1    STEP: 25   LOSS: 3.192826509475708\n",
            "EPOCH: 1    STEP: 50   LOSS: 3.1964797973632812\n",
            "EPOCH: 1    STEP: 75   LOSS: 3.231184959411621\n",
            "EPOCH: 1    STEP: 100   LOSS: 3.215151071548462\n",
            "EPOCH: 1    STEP: 125   LOSS: 3.171006679534912\n",
            "EPOCH: 1    STEP: 150   LOSS: 3.0044960975646973\n",
            "EPOCH: 1    STEP: 175   LOSS: 2.9398000240325928\n",
            "EPOCH: 1    STEP: 200   LOSS: 2.7872278690338135\n",
            "EPOCH: 1    STEP: 225   LOSS: 2.7979300022125244\n",
            "EPOCH: 1    STEP: 250   LOSS: 2.648042917251587\n",
            "EPOCH: 1    STEP: 275   LOSS: 2.5240023136138916\n",
            "EPOCH: 1    STEP: 300   LOSS: 2.4200057983398438\n",
            "EPOCH: 1    STEP: 325   LOSS: 2.3248562812805176\n",
            "EPOCH: 1    STEP: 350   LOSS: 2.239218235015869\n",
            "EPOCH: 1    STEP: 375   LOSS: 2.21869158744812\n",
            "EPOCH: 2    STEP: 400   LOSS: 2.1812283992767334\n",
            "EPOCH: 2    STEP: 425   LOSS: 2.116616725921631\n",
            "EPOCH: 2    STEP: 450   LOSS: 2.111952304840088\n",
            "EPOCH: 2    STEP: 475   LOSS: 2.062443733215332\n",
            "EPOCH: 2    STEP: 500   LOSS: 2.0265390872955322\n",
            "EPOCH: 2    STEP: 525   LOSS: 2.053476333618164\n",
            "EPOCH: 2    STEP: 550   LOSS: 1.986167550086975\n",
            "EPOCH: 2    STEP: 575   LOSS: 1.9503731727600098\n",
            "EPOCH: 2    STEP: 600   LOSS: 1.9383996725082397\n",
            "EPOCH: 2    STEP: 625   LOSS: 1.9365513324737549\n",
            "EPOCH: 2    STEP: 650   LOSS: 1.9072741270065308\n",
            "EPOCH: 2    STEP: 675   LOSS: 1.9038907289505005\n",
            "EPOCH: 2    STEP: 700   LOSS: 1.8946013450622559\n",
            "EPOCH: 2    STEP: 725   LOSS: 1.830323338508606\n",
            "EPOCH: 2    STEP: 750   LOSS: 1.8174837827682495\n",
            "EPOCH: 3    STEP: 775   LOSS: 1.871086835861206\n",
            "EPOCH: 3    STEP: 800   LOSS: 1.79494047164917\n",
            "EPOCH: 3    STEP: 825   LOSS: 1.800806999206543\n",
            "EPOCH: 3    STEP: 850   LOSS: 1.7724473476409912\n",
            "EPOCH: 3    STEP: 875   LOSS: 1.7730451822280884\n",
            "EPOCH: 3    STEP: 900   LOSS: 1.7683161497116089\n",
            "EPOCH: 3    STEP: 925   LOSS: 1.7212942838668823\n",
            "EPOCH: 3    STEP: 950   LOSS: 1.7207738161087036\n",
            "EPOCH: 3    STEP: 975   LOSS: 1.696137547492981\n",
            "EPOCH: 3    STEP: 1000   LOSS: 1.6985185146331787\n",
            "EPOCH: 3    STEP: 1025   LOSS: 1.7327078580856323\n",
            "EPOCH: 3    STEP: 1050   LOSS: 1.7512295246124268\n",
            "EPOCH: 3    STEP: 1075   LOSS: 1.7213592529296875\n",
            "EPOCH: 3    STEP: 1100   LOSS: 1.703465461730957\n",
            "EPOCH: 3    STEP: 1125   LOSS: 1.6678321361541748\n",
            "EPOCH: 4    STEP: 1150   LOSS: 1.6554670333862305\n",
            "EPOCH: 4    STEP: 1175   LOSS: 1.6509507894515991\n",
            "EPOCH: 4    STEP: 1200   LOSS: 1.6475671529769897\n",
            "EPOCH: 4    STEP: 1225   LOSS: 1.6415170431137085\n",
            "EPOCH: 4    STEP: 1250   LOSS: 1.612492322921753\n",
            "EPOCH: 4    STEP: 1275   LOSS: 1.6592228412628174\n",
            "EPOCH: 4    STEP: 1300   LOSS: 1.6117953062057495\n",
            "EPOCH: 4    STEP: 1325   LOSS: 1.628198266029358\n",
            "EPOCH: 4    STEP: 1350   LOSS: 1.6120429039001465\n",
            "EPOCH: 4    STEP: 1375   LOSS: 1.5633028745651245\n",
            "EPOCH: 4    STEP: 1400   LOSS: 1.6148661375045776\n",
            "EPOCH: 4    STEP: 1425   LOSS: 1.587085247039795\n",
            "EPOCH: 4    STEP: 1450   LOSS: 1.6171386241912842\n",
            "EPOCH: 4    STEP: 1475   LOSS: 1.5330824851989746\n",
            "EPOCH: 4    STEP: 1500   LOSS: 1.5890061855316162\n",
            "EPOCH: 4    STEP: 1525   LOSS: 1.5842775106430054\n",
            "EPOCH: 5    STEP: 1550   LOSS: 1.5813299417495728\n",
            "EPOCH: 5    STEP: 1575   LOSS: 1.5194091796875\n",
            "EPOCH: 5    STEP: 1600   LOSS: 1.5547118186950684\n",
            "EPOCH: 5    STEP: 1625   LOSS: 1.5808836221694946\n",
            "EPOCH: 5    STEP: 1650   LOSS: 1.532894253730774\n",
            "EPOCH: 5    STEP: 1675   LOSS: 1.4905884265899658\n",
            "EPOCH: 5    STEP: 1700   LOSS: 1.5276341438293457\n",
            "EPOCH: 5    STEP: 1725   LOSS: 1.559302806854248\n",
            "EPOCH: 5    STEP: 1750   LOSS: 1.5304985046386719\n",
            "EPOCH: 5    STEP: 1775   LOSS: 1.5189212560653687\n",
            "EPOCH: 5    STEP: 1800   LOSS: 1.4599069356918335\n",
            "EPOCH: 5    STEP: 1825   LOSS: 1.5272353887557983\n",
            "EPOCH: 5    STEP: 1850   LOSS: 1.502794623374939\n",
            "EPOCH: 5    STEP: 1875   LOSS: 1.4864953756332397\n",
            "EPOCH: 5    STEP: 1900   LOSS: 1.5113422870635986\n",
            "EPOCH: 6    STEP: 1925   LOSS: 1.535485029220581\n",
            "EPOCH: 6    STEP: 1950   LOSS: 1.487721562385559\n",
            "EPOCH: 6    STEP: 1975   LOSS: 1.496289849281311\n",
            "EPOCH: 6    STEP: 2000   LOSS: 1.4771511554718018\n",
            "EPOCH: 6    STEP: 2025   LOSS: 1.4603590965270996\n",
            "EPOCH: 6    STEP: 2050   LOSS: 1.454127550125122\n",
            "EPOCH: 6    STEP: 2075   LOSS: 1.4320399761199951\n",
            "EPOCH: 6    STEP: 2100   LOSS: 1.456922173500061\n",
            "EPOCH: 6    STEP: 2125   LOSS: 1.4095338582992554\n",
            "EPOCH: 6    STEP: 2150   LOSS: 1.4883495569229126\n",
            "EPOCH: 6    STEP: 2175   LOSS: 1.4853951930999756\n",
            "EPOCH: 6    STEP: 2200   LOSS: 1.4803776741027832\n",
            "EPOCH: 6    STEP: 2225   LOSS: 1.4533379077911377\n",
            "EPOCH: 6    STEP: 2250   LOSS: 1.4502562284469604\n",
            "EPOCH: 6    STEP: 2275   LOSS: 1.4309948682785034\n",
            "EPOCH: 7    STEP: 2300   LOSS: 1.4207464456558228\n",
            "EPOCH: 7    STEP: 2325   LOSS: 1.4037981033325195\n",
            "EPOCH: 7    STEP: 2350   LOSS: 1.4194159507751465\n",
            "EPOCH: 7    STEP: 2375   LOSS: 1.4515670537948608\n",
            "EPOCH: 7    STEP: 2400   LOSS: 1.4053114652633667\n",
            "EPOCH: 7    STEP: 2425   LOSS: 1.4252629280090332\n",
            "EPOCH: 7    STEP: 2450   LOSS: 1.4183789491653442\n",
            "EPOCH: 7    STEP: 2475   LOSS: 1.425305962562561\n",
            "EPOCH: 7    STEP: 2500   LOSS: 1.3775728940963745\n",
            "EPOCH: 7    STEP: 2525   LOSS: 1.3703796863555908\n",
            "EPOCH: 7    STEP: 2550   LOSS: 1.4499326944351196\n",
            "EPOCH: 7    STEP: 2575   LOSS: 1.386948585510254\n",
            "EPOCH: 7    STEP: 2600   LOSS: 1.4109781980514526\n",
            "EPOCH: 7    STEP: 2625   LOSS: 1.3510353565216064\n",
            "EPOCH: 7    STEP: 2650   LOSS: 1.3960046768188477\n",
            "EPOCH: 8    STEP: 2675   LOSS: 1.4787074327468872\n",
            "EPOCH: 8    STEP: 2700   LOSS: 1.3902662992477417\n",
            "EPOCH: 8    STEP: 2725   LOSS: 1.3816845417022705\n",
            "EPOCH: 8    STEP: 2750   LOSS: 1.4032849073410034\n",
            "EPOCH: 8    STEP: 2775   LOSS: 1.359063744544983\n",
            "EPOCH: 8    STEP: 2800   LOSS: 1.3787678480148315\n",
            "EPOCH: 8    STEP: 2825   LOSS: 1.3663231134414673\n",
            "EPOCH: 8    STEP: 2850   LOSS: 1.3210638761520386\n",
            "EPOCH: 8    STEP: 2875   LOSS: 1.3828753232955933\n",
            "EPOCH: 8    STEP: 2900   LOSS: 1.3562355041503906\n",
            "EPOCH: 8    STEP: 2925   LOSS: 1.3704551458358765\n",
            "EPOCH: 8    STEP: 2950   LOSS: 1.3978681564331055\n",
            "EPOCH: 8    STEP: 2975   LOSS: 1.3940997123718262\n",
            "EPOCH: 8    STEP: 3000   LOSS: 1.3651198148727417\n",
            "EPOCH: 8    STEP: 3025   LOSS: 1.3912080526351929\n",
            "EPOCH: 8    STEP: 3050   LOSS: 1.3788642883300781\n",
            "EPOCH: 9    STEP: 3075   LOSS: 1.3403733968734741\n",
            "EPOCH: 9    STEP: 3100   LOSS: 1.329579472541809\n",
            "EPOCH: 9    STEP: 3125   LOSS: 1.371355414390564\n",
            "EPOCH: 9    STEP: 3150   LOSS: 1.392771601676941\n",
            "EPOCH: 9    STEP: 3175   LOSS: 1.3676191568374634\n",
            "EPOCH: 9    STEP: 3200   LOSS: 1.2994617223739624\n",
            "EPOCH: 9    STEP: 3225   LOSS: 1.3773956298828125\n",
            "EPOCH: 9    STEP: 3250   LOSS: 1.3442778587341309\n",
            "EPOCH: 9    STEP: 3275   LOSS: 1.382700800895691\n",
            "EPOCH: 9    STEP: 3300   LOSS: 1.3500816822052002\n",
            "EPOCH: 9    STEP: 3325   LOSS: 1.2895808219909668\n",
            "EPOCH: 9    STEP: 3350   LOSS: 1.3557543754577637\n",
            "EPOCH: 9    STEP: 3375   LOSS: 1.3620067834854126\n",
            "EPOCH: 9    STEP: 3400   LOSS: 1.31312894821167\n",
            "EPOCH: 9    STEP: 3425   LOSS: 1.3004423379898071\n",
            "EPOCH: 10    STEP: 3450   LOSS: 1.3624622821807861\n",
            "EPOCH: 10    STEP: 3475   LOSS: 1.3112459182739258\n",
            "EPOCH: 10    STEP: 3500   LOSS: 1.331393837928772\n",
            "EPOCH: 10    STEP: 3525   LOSS: 1.315853238105774\n",
            "EPOCH: 10    STEP: 3550   LOSS: 1.347792625427246\n",
            "EPOCH: 10    STEP: 3575   LOSS: 1.332983374595642\n",
            "EPOCH: 10    STEP: 3600   LOSS: 1.2922941446304321\n",
            "EPOCH: 10    STEP: 3625   LOSS: 1.3035963773727417\n",
            "EPOCH: 10    STEP: 3650   LOSS: 1.2897861003875732\n",
            "EPOCH: 10    STEP: 3675   LOSS: 1.3440661430358887\n",
            "EPOCH: 10    STEP: 3700   LOSS: 1.3249154090881348\n",
            "EPOCH: 10    STEP: 3725   LOSS: 1.3426964282989502\n",
            "EPOCH: 10    STEP: 3750   LOSS: 1.3124340772628784\n",
            "EPOCH: 10    STEP: 3775   LOSS: 1.2853753566741943\n",
            "EPOCH: 10    STEP: 3800   LOSS: 1.3072903156280518\n",
            "EPOCH: 11    STEP: 3825   LOSS: 1.31400728225708\n",
            "EPOCH: 11    STEP: 3850   LOSS: 1.3227022886276245\n",
            "EPOCH: 11    STEP: 3875   LOSS: 1.3304426670074463\n",
            "EPOCH: 11    STEP: 3900   LOSS: 1.2957801818847656\n",
            "EPOCH: 11    STEP: 3925   LOSS: 1.3227556943893433\n",
            "EPOCH: 11    STEP: 3950   LOSS: 1.3208767175674438\n",
            "EPOCH: 11    STEP: 3975   LOSS: 1.3149232864379883\n",
            "EPOCH: 11    STEP: 4000   LOSS: 1.3054274320602417\n",
            "EPOCH: 11    STEP: 4025   LOSS: 1.3441699743270874\n",
            "EPOCH: 11    STEP: 4050   LOSS: 1.3135936260223389\n",
            "EPOCH: 11    STEP: 4075   LOSS: 1.326392650604248\n",
            "EPOCH: 11    STEP: 4100   LOSS: 1.3357934951782227\n",
            "EPOCH: 11    STEP: 4125   LOSS: 1.3135064840316772\n",
            "EPOCH: 11    STEP: 4150   LOSS: 1.3007961511611938\n",
            "EPOCH: 11    STEP: 4175   LOSS: 1.2812063694000244\n",
            "EPOCH: 11    STEP: 4200   LOSS: 1.3275082111358643\n",
            "EPOCH: 12    STEP: 4225   LOSS: 1.3436062335968018\n",
            "EPOCH: 12    STEP: 4250   LOSS: 1.2792997360229492\n",
            "EPOCH: 12    STEP: 4275   LOSS: 1.309820532798767\n",
            "EPOCH: 12    STEP: 4300   LOSS: 1.283759593963623\n",
            "EPOCH: 12    STEP: 4325   LOSS: 1.2933601140975952\n",
            "EPOCH: 12    STEP: 4350   LOSS: 1.2255587577819824\n",
            "EPOCH: 12    STEP: 4375   LOSS: 1.2523442506790161\n",
            "EPOCH: 12    STEP: 4400   LOSS: 1.3058509826660156\n",
            "EPOCH: 12    STEP: 4425   LOSS: 1.2680444717407227\n",
            "EPOCH: 12    STEP: 4450   LOSS: 1.2764323949813843\n",
            "EPOCH: 12    STEP: 4475   LOSS: 1.2687283754348755\n",
            "EPOCH: 12    STEP: 4500   LOSS: 1.2721412181854248\n",
            "EPOCH: 12    STEP: 4525   LOSS: 1.2588379383087158\n",
            "EPOCH: 12    STEP: 4550   LOSS: 1.2570393085479736\n",
            "EPOCH: 12    STEP: 4575   LOSS: 1.2744232416152954\n",
            "EPOCH: 13    STEP: 4600   LOSS: 1.3140861988067627\n",
            "EPOCH: 13    STEP: 4625   LOSS: 1.242388129234314\n",
            "EPOCH: 13    STEP: 4650   LOSS: 1.31178617477417\n",
            "EPOCH: 13    STEP: 4675   LOSS: 1.3028606176376343\n",
            "EPOCH: 13    STEP: 4700   LOSS: 1.2491406202316284\n",
            "EPOCH: 13    STEP: 4725   LOSS: 1.2491812705993652\n",
            "EPOCH: 13    STEP: 4750   LOSS: 1.2704429626464844\n",
            "EPOCH: 13    STEP: 4775   LOSS: 1.3187565803527832\n",
            "EPOCH: 13    STEP: 4800   LOSS: 1.2814795970916748\n",
            "EPOCH: 13    STEP: 4825   LOSS: 1.2620480060577393\n",
            "EPOCH: 13    STEP: 4850   LOSS: 1.2645761966705322\n",
            "EPOCH: 13    STEP: 4875   LOSS: 1.3330717086791992\n",
            "EPOCH: 13    STEP: 4900   LOSS: 1.299381136894226\n",
            "EPOCH: 13    STEP: 4925   LOSS: 1.2879606485366821\n",
            "EPOCH: 13    STEP: 4950   LOSS: 1.2704975605010986\n",
            "EPOCH: 14    STEP: 4975   LOSS: 1.297835350036621\n",
            "EPOCH: 14    STEP: 5000   LOSS: 1.2640438079833984\n",
            "EPOCH: 14    STEP: 5025   LOSS: 1.2900996208190918\n",
            "EPOCH: 14    STEP: 5050   LOSS: 1.2844529151916504\n",
            "EPOCH: 14    STEP: 5075   LOSS: 1.2604124546051025\n",
            "EPOCH: 14    STEP: 5100   LOSS: 1.2384698390960693\n",
            "EPOCH: 14    STEP: 5125   LOSS: 1.2778955698013306\n",
            "EPOCH: 14    STEP: 5150   LOSS: 1.2669289112091064\n",
            "EPOCH: 14    STEP: 5175   LOSS: 1.251866102218628\n",
            "EPOCH: 14    STEP: 5200   LOSS: 1.239098072052002\n",
            "EPOCH: 14    STEP: 5225   LOSS: 1.279668927192688\n",
            "EPOCH: 14    STEP: 5250   LOSS: 1.2737820148468018\n",
            "EPOCH: 14    STEP: 5275   LOSS: 1.2662676572799683\n",
            "EPOCH: 14    STEP: 5300   LOSS: 1.2352374792099\n",
            "EPOCH: 14    STEP: 5325   LOSS: 1.2465518712997437\n",
            "EPOCH: 15    STEP: 5350   LOSS: 1.2196844816207886\n",
            "EPOCH: 15    STEP: 5375   LOSS: 1.2515052556991577\n",
            "EPOCH: 15    STEP: 5400   LOSS: 1.2873317003250122\n",
            "EPOCH: 15    STEP: 5425   LOSS: 1.2828052043914795\n",
            "EPOCH: 15    STEP: 5450   LOSS: 1.251579761505127\n",
            "EPOCH: 15    STEP: 5475   LOSS: 1.2664505243301392\n",
            "EPOCH: 15    STEP: 5500   LOSS: 1.2408374547958374\n",
            "EPOCH: 15    STEP: 5525   LOSS: 1.2114932537078857\n",
            "EPOCH: 15    STEP: 5550   LOSS: 1.24503755569458\n",
            "EPOCH: 15    STEP: 5575   LOSS: 1.2274242639541626\n",
            "EPOCH: 15    STEP: 5600   LOSS: 1.2704055309295654\n",
            "EPOCH: 15    STEP: 5625   LOSS: 1.2759602069854736\n",
            "EPOCH: 15    STEP: 5650   LOSS: 1.2537347078323364\n",
            "EPOCH: 15    STEP: 5675   LOSS: 1.2881054878234863\n",
            "EPOCH: 15    STEP: 5700   LOSS: 1.3056329488754272\n",
            "EPOCH: 15    STEP: 5725   LOSS: 1.2534080743789673\n",
            "EPOCH: 16    STEP: 5750   LOSS: 1.2604262828826904\n",
            "EPOCH: 16    STEP: 5775   LOSS: 1.2516340017318726\n",
            "EPOCH: 16    STEP: 5800   LOSS: 1.2901307344436646\n",
            "EPOCH: 16    STEP: 5825   LOSS: 1.2796738147735596\n",
            "EPOCH: 16    STEP: 5850   LOSS: 1.2652159929275513\n",
            "EPOCH: 16    STEP: 5875   LOSS: 1.2072868347167969\n",
            "EPOCH: 16    STEP: 5900   LOSS: 1.253635048866272\n",
            "EPOCH: 16    STEP: 5925   LOSS: 1.2273036241531372\n",
            "EPOCH: 16    STEP: 5950   LOSS: 1.291191816329956\n",
            "EPOCH: 16    STEP: 5975   LOSS: 1.2760851383209229\n",
            "EPOCH: 16    STEP: 6000   LOSS: 1.2135757207870483\n",
            "EPOCH: 16    STEP: 6025   LOSS: 1.262122631072998\n",
            "EPOCH: 16    STEP: 6050   LOSS: 1.2488298416137695\n",
            "EPOCH: 16    STEP: 6075   LOSS: 1.2005866765975952\n",
            "EPOCH: 16    STEP: 6100   LOSS: 1.2231054306030273\n",
            "EPOCH: 17    STEP: 6125   LOSS: 1.2582085132598877\n",
            "EPOCH: 17    STEP: 6150   LOSS: 1.2237473726272583\n",
            "EPOCH: 17    STEP: 6175   LOSS: 1.243066430091858\n",
            "EPOCH: 17    STEP: 6200   LOSS: 1.2393832206726074\n",
            "EPOCH: 17    STEP: 6225   LOSS: 1.2141114473342896\n",
            "EPOCH: 17    STEP: 6250   LOSS: 1.210448980331421\n",
            "EPOCH: 17    STEP: 6275   LOSS: 1.1966453790664673\n",
            "EPOCH: 17    STEP: 6300   LOSS: 1.2331701517105103\n",
            "EPOCH: 17    STEP: 6325   LOSS: 1.2188664674758911\n",
            "EPOCH: 17    STEP: 6350   LOSS: 1.2174267768859863\n",
            "EPOCH: 17    STEP: 6375   LOSS: 1.2440071105957031\n",
            "EPOCH: 17    STEP: 6400   LOSS: 1.2724376916885376\n",
            "EPOCH: 17    STEP: 6425   LOSS: 1.21860933303833\n",
            "EPOCH: 17    STEP: 6450   LOSS: 1.246097445487976\n",
            "EPOCH: 17    STEP: 6475   LOSS: 1.2378679513931274\n",
            "EPOCH: 18    STEP: 6500   LOSS: 1.2068442106246948\n",
            "EPOCH: 18    STEP: 6525   LOSS: 1.2224762439727783\n",
            "EPOCH: 18    STEP: 6550   LOSS: 1.2146003246307373\n",
            "EPOCH: 18    STEP: 6575   LOSS: 1.2418603897094727\n",
            "EPOCH: 18    STEP: 6600   LOSS: 1.2722045183181763\n",
            "EPOCH: 18    STEP: 6625   LOSS: 1.2293267250061035\n",
            "EPOCH: 18    STEP: 6650   LOSS: 1.230069637298584\n",
            "EPOCH: 18    STEP: 6675   LOSS: 1.2352327108383179\n",
            "EPOCH: 18    STEP: 6700   LOSS: 1.2109801769256592\n",
            "EPOCH: 18    STEP: 6725   LOSS: 1.2417372465133667\n",
            "EPOCH: 18    STEP: 6750   LOSS: 1.230812907218933\n",
            "EPOCH: 18    STEP: 6775   LOSS: 1.246995210647583\n",
            "EPOCH: 18    STEP: 6800   LOSS: 1.2752199172973633\n",
            "EPOCH: 18    STEP: 6825   LOSS: 1.2228342294692993\n",
            "EPOCH: 18    STEP: 6850   LOSS: 1.2189574241638184\n",
            "EPOCH: 18    STEP: 6875   LOSS: 1.2259808778762817\n",
            "EPOCH: 19    STEP: 6900   LOSS: 1.2419757843017578\n",
            "EPOCH: 19    STEP: 6925   LOSS: 1.2187210321426392\n",
            "EPOCH: 19    STEP: 6950   LOSS: 1.2368972301483154\n",
            "EPOCH: 19    STEP: 6975   LOSS: 1.2053974866867065\n",
            "EPOCH: 19    STEP: 7000   LOSS: 1.235959529876709\n",
            "EPOCH: 19    STEP: 7025   LOSS: 1.2017273902893066\n",
            "EPOCH: 19    STEP: 7050   LOSS: 1.1739211082458496\n",
            "EPOCH: 19    STEP: 7075   LOSS: 1.2428096532821655\n",
            "EPOCH: 19    STEP: 7100   LOSS: 1.2379519939422607\n",
            "EPOCH: 19    STEP: 7125   LOSS: 1.2125787734985352\n",
            "EPOCH: 19    STEP: 7150   LOSS: 1.2264962196350098\n",
            "EPOCH: 19    STEP: 7175   LOSS: 1.2231149673461914\n",
            "EPOCH: 19    STEP: 7200   LOSS: 1.2543206214904785\n",
            "EPOCH: 19    STEP: 7225   LOSS: 1.2237244844436646\n",
            "EPOCH: 19    STEP: 7250   LOSS: 1.2269173860549927\n",
            "EPOCH: 20    STEP: 7275   LOSS: 1.208367943763733\n",
            "EPOCH: 20    STEP: 7300   LOSS: 1.2028518915176392\n",
            "EPOCH: 20    STEP: 7325   LOSS: 1.2427786588668823\n",
            "EPOCH: 20    STEP: 7350   LOSS: 1.2414546012878418\n",
            "EPOCH: 20    STEP: 7375   LOSS: 1.2247724533081055\n",
            "EPOCH: 20    STEP: 7400   LOSS: 1.160186767578125\n",
            "EPOCH: 20    STEP: 7425   LOSS: 1.2467416524887085\n",
            "EPOCH: 20    STEP: 7450   LOSS: 1.2101631164550781\n",
            "EPOCH: 20    STEP: 7475   LOSS: 1.2165333032608032\n",
            "EPOCH: 20    STEP: 7500   LOSS: 1.2333533763885498\n",
            "EPOCH: 20    STEP: 7525   LOSS: 1.2228392362594604\n",
            "EPOCH: 20    STEP: 7550   LOSS: 1.214686393737793\n",
            "EPOCH: 20    STEP: 7575   LOSS: 1.2465660572052002\n",
            "EPOCH: 20    STEP: 7600   LOSS: 1.252036452293396\n",
            "EPOCH: 20    STEP: 7625   LOSS: 1.2091753482818604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUdm5LZRGBIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/NLP_with_pytorch/512Hidden3Layers.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoybqoQHkmlB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcb08dba-787b-4640-f0df-f6cf225e3bfb"
      },
      "source": [
        "model = Model(all_characters, num_hidden=512, num_layers=3, drop_prob=0.5, use_gpu=use_gpu)\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/Colab Notebooks/NLP_with_pytorch/512Hidden3Layers.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcmM81wFdnas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_char(model, char, hidden=None, k=1):\n",
        "        \n",
        "        encoded_text = model.encoder[char]\n",
        "        encoded_text = np.array([[encoded_text]])\n",
        "        encoded_text = one_hot_encoder(encoded_text, len(model.all_characters))\n",
        "        inputs = torch.from_numpy(encoded_text)\n",
        "        \n",
        "        if(model.use_gpu):\n",
        "            inputs = inputs.float().cuda()\n",
        "        \n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "        lstm_out, hidden = model(inputs, hidden)\n",
        "        probs = F.softmax(lstm_out, dim=1).data\n",
        "        \n",
        "        if(model.use_gpu):\n",
        "            probs = probs.cpu()\n",
        "        \n",
        "        probs, index_positions = probs.topk(k)\n",
        "        index_positions = index_positions.numpy().squeeze()\n",
        "        probs = probs.numpy().flatten()\n",
        "        probs = probs/probs.sum()\n",
        "        char = np.random.choice(index_positions, p=probs)\n",
        "       \n",
        "        return model.decoder[char], hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvGnhr-OoBXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, size, seed='The', k=1):\n",
        "    \n",
        "    if(model.use_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "    model.eval()\n",
        "    output_chars = [c for c in seed]\n",
        "    hidden = model.hidden_state(1)\n",
        "    \n",
        "    for char in seed:\n",
        "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "    output_chars.append(char)\n",
        "    \n",
        "    for i in range(size):\n",
        "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "        output_chars.append(char)\n",
        "    \n",
        "    return ''.join(output_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPWqfF9ztroi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shakey = predict_sequence(model, 10000, seed=\"The \", k=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGs95_7nt8Ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c24b5854-697e-42ef-8d67-6227df1fc9d5"
      },
      "source": [
        "print(shakey)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The three thing the woman will\n",
            "    There were not beaten.\n",
            "  Pedro. The world will be a mind, that the son of her side,  \n",
            "    Where I have seen't to th' man.\n",
            "  Ham. I wish my life.\n",
            "    Then what this short on thee, that shall I stand\n",
            "    When thou art a most star and so much son.\n",
            "    There is no man at marching too, and the story\n",
            "    Were to the chamber, and the words of mine,\n",
            "    Whose presencious soul is all and this  \n",
            "    A prophotition to the will and sorrow.\n",
            "    There in mine ears will say this seat thou wouldst\n",
            "    The sun that he should be the storm of man\n",
            "    That shall be made and be the winder to him;\n",
            "    Therefore be there as this so fair as hell.\n",
            "    If they should see them all, I was not sent,\n",
            "    When I have talk'd to be a service to\n",
            "    The word of the chair, which this thou seest her,\n",
            "    And there in his shaming thought of honesty,\n",
            "    And see he hath but born and stores as stand,\n",
            "    As if his honour was the contents of my son.\n",
            "    I am too string and, as I shall be true.\n",
            "\n",
            "                           Enter PANDARUS\n",
            "\n",
            "  PRINCE. I have seen her and so sorry to thy father.\n",
            "  SECOND LORD. What in my soldiers had I that should be so,\n",
            "    To see the wish that he hath stand'd too leave?\n",
            "    What war a storm of heaven is the means of too,\n",
            "    The words of man in his charge of my soul,\n",
            "    And to the woman to thy heart to tell you?\n",
            "    This is a monument, when thou art sure,\n",
            "    The man and the strange other was to bring.\n",
            "    Then, this is a soul of my heart, and then,\n",
            "    The men to stay.\n",
            "  CAESAR. I have no longer to his hand.          Exit SALISBURY\n",
            "  SECOND GENTLEMAN. I am some strength.\n",
            "  PRINCE. To take her to my son to th' ear.\n",
            "    What is your head? Will you think you have straight?\n",
            "  CONSTANCE. What, who should help?\n",
            "\n",
            "                       Re-enter BRAKENBURY\n",
            "\n",
            "  CLARENCE. I am a soldier, thank you all the most too much,\n",
            "    And so the subject is a more of my heart.\n",
            "  KING RICHARD. The men thou wast to bear the state of mind.\n",
            "  KATHARINE. Though this is there to be the son and honour,\n",
            "    And that this warrant of this beauty things\n",
            "    Thou show'st my head and men to be a soldier,\n",
            "    And strays thee with a brains to be the war.\n",
            "    I was a form of honour of the state\n",
            "    Whose stroke the wind in my son in the strange\n",
            "    This strange of heaven and men, and he will bear.\n",
            "    What would thou seest? I'll speak them and to see.\n",
            "    To be the men that hath been any true.\n",
            "  COMINIUS. I will, and see her strongly and a monster,\n",
            "    That what this world the means that shall be then.\n",
            "    If the man touch he seems a most best tongue.\n",
            "    Thou shalt not hear them all as thou hast sent to make him.\n",
            "    What is the most success of mercy with\n",
            "    Within the most the strange time it well did so?\n",
            "    Where is this truth that had the mortal honour\n",
            "    That shall be tried out to the matter?\n",
            "    This is the man. I have the most thousand storm\n",
            "    Will stay into the man to thee.\n",
            "    The subject of my head to say I had tell me,\n",
            "    That we shall be the subject, where your son,\n",
            "    To see the stream of heaven that I have been\n",
            "    The strength of the common things and the ward doth\n",
            "    As thou hast not been so such things as soul,\n",
            "    With think the counterations, and the state\n",
            "    To be a strong and brother to the father\n",
            "    That strike them too much. I had been an angel,\n",
            "    They will be so storm'd, and then their hands are both\n",
            "    To stranger thee.\n",
            "  MARCIUS. To save your house with hour, and he is not\n",
            "    And show'd himself to be a field of thing,\n",
            "    The sending true to see this strength of heart.\n",
            "    Think you and show'st to the servant of heart\n",
            "    Where the meant is the water.\n",
            "  SUFFOLK. What make you there?\n",
            "    If thou art so, the strong time is to see. I have\n",
            "    I see them well that I will seem the wit\n",
            "    To think his son in the state of this soul thought.\n",
            "    Well say, a strength as the main thing they walk,\n",
            "    And so the word it is this soul, a most\n",
            "    With their best true and truth of heavy son,\n",
            "    The state as hand of them at me a state,\n",
            "    And, who is so much only to the counterforce\n",
            "    That the soul thou hast been a soldier's soul.\n",
            "    If your side stand and think the many and\n",
            "    We have been the way to be strike to think\n",
            "    As thou art the most pardon of this soul.\n",
            "    We she is so, and we will be a man\n",
            "    And stand the contrary to me with my heart.\n",
            "    I am a maid and think they will. This was\n",
            "    To be a man, to breed the world in this\n",
            "    As I was soul of this to see them straight,\n",
            "    To th' strength that they were the subject to the state.\n",
            "    This would I have the sentence the with thing.\n",
            "    I am a merry strong thing that the world,\n",
            "    Which is a strain'd truth of the state and shore.\n",
            "    This, though you shall be still and this they walk\n",
            "    We will be so strike on mine own strong faith.\n",
            "    Thou hast but an earth a men and sheep,\n",
            "    Which it was there to bear. The world is too man.\n",
            "\n",
            "           Enter COSTARD and COMINIUS and LARTIUS\n",
            "\n",
            "  CASSIUS. Why, there is to be so much.  \n",
            "    I have no man in him to take the wind\n",
            "    As they shall stay in his body to the charge\n",
            "    As which I shall be made that seem in his\n",
            "    To take a painting to me. If thou dost stand,\n",
            "    The most and tributious time that should set thee,\n",
            "    That we have seen the station to-day to him,\n",
            "    We will not speak to me, and the soul of\n",
            "    The more of this the man on her be heard\n",
            "    The state of their stating on the confusion.  \n",
            "    The wind of twice in that she holds a fair\n",
            "    Than hath the sea to-day.\n",
            "  CORIOLANUS. I have not been.\n",
            "  CASSIUS. I am a son that was a sea and the mark\n",
            "    Than which is success's to the world is now\n",
            "    That the substracted that the wind of son\n",
            "    Thou hast no man the way. If I have beat him\n",
            "    To see this strength that I have bear the strength\n",
            "    To be a fool, and this is a man to thee;\n",
            "    And should to strike a part of that stomach too,\n",
            "    And this too much as still in this time had to see\n",
            "    A strengte of the wars of this time on me.\n",
            "    To be thy state. What is the sun to me,\n",
            "    Which think you to the soul, and the most deed?  \n",
            "    I am to be a state to this so much\n",
            "    That this was breathed.\n",
            "  CLEOPATRA. What, a stand and more?\n",
            "    To this man shall this shall they should be this.\n",
            "    Why then, my lord, I have sent him now to stay.\n",
            "    What would I show me there in many to me?\n",
            "    I have but triumph'd in the mother to the King.\n",
            "    The wind of this world will be meet this blest,\n",
            "    The strength of that thou hadst need a fool and honour,\n",
            "    And the songer of the world the story strikes\n",
            "    The wisdess of the world as their shorts still,\n",
            "    And will not sent thee when. Thou hast not seen\n",
            "    Than when you have the more a man that he was\n",
            "    With her to be the stone of merry will,\n",
            "    That we have see his beauty of this fire\n",
            "    And send me all the mouth, and his son of\n",
            "    The word is nothing. The ways of hell have breath'd.\n",
            "    What, when the strong art thou to me? What said\n",
            "    The state of the summer, and that the man,\n",
            "    And to my life the means? Why then thou shouldst both her\n",
            "    Into my hores, which in their heads are bries,\n",
            "    And shall I had to say I shall but see.\n",
            "    The matters will I see thy former sight.\n",
            "    If I have brought me to their solemns sound.\n",
            "    If I was not the sense and strangers and\n",
            "    To be and my sons of her, to th' earth.\n",
            "    The most man then and so I would not be\n",
            "    This world is not that think out of his hand.           Exeunt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\n",
            "SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\n",
            "PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\n",
            "WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\n",
            "DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\n",
            "PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\n",
            "COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\n",
            "SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\n",
            "\n",
            "\n",
            "\n",
            "ACT V. Scene I.\n",
            "The King and Cassio.\n",
            "\n",
            "Enter Castle.\n",
            "\n",
            "  Prito. Wilt thou see that worthy signior whose truth is a soldier?\n",
            "  Hot. What should I have a man and my soul to the sons? What art\n",
            "    an answer to my sister's tongue? The worst, I hope to see\n",
            "    them, to be the morning that the marriage of the son of Falstaff,\n",
            "    tell me that I am a state, when I have there to bear the morning in\n",
            "    the matter, and a mean of his best state as I hope to tell thee a morn\n",
            "    of the sense, that I have been a stranger, when they were a month of\n",
            "    man of his best.\n",
            "  Ham. What, when this?\n",
            "  Pol. This is this storm. Then there's the state.\n",
            "  Claud. That we have been this storm of him. Why!\n",
            "  Prant. The story of my head is the most shame that I have been to take\n",
            "    a sons, while I have been the soldier. That I had been a matter in\n",
            "    the soul as they say I was think you will the story, and the\n",
            "    courtesy of the mother, when thou shouldst be a soldier to his side\n",
            "    than the maiden the stronger of this treacherous thousand tongue to the moon\n",
            "    and heaven of heaven, and to this storm, that I was sent\n",
            "    there were no letter, who is their state at the sense of the\n",
            "    first of the wits and the commandment to his head and honourable\n",
            "    strange actions of the will, and his boy she's, to be so sold of him.\n",
            "    If thou art that I would not stand to the care the woold\n",
            "    of the charge, and this they are the more than thou, so much to\n",
            "    still but see my love in the more as a stranger of his stranger\n",
            "    of this time.\n",
            "  Ham. I had a strength of house to seek him to her strong offender to him\n",
            "    to help, and with the most thought of the castle is not born than I\n",
            "    am not the meaning one of this to my bettire. To happily that I\n",
            "    have not sent to the merchantine a maiden.\n",
            "  Con. I will be so much of made the child a strength to serve my\n",
            "    strength of my lawful.\n",
            "  Ham. Well, my lord.\n",
            "  Ham. Why, sir, we'll bring the men and the confession in my stones.\n",
            "  Prince. I have been so so tongue into the soldiers, with a maid o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXsQWar2yzAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8824752-ebb2-4daa-e0ec-85ab858f1a54"
      },
      "source": [
        "f = open('/content/gdrive/My Drive/Colab Notebooks/NLP_with_pytorch/Data/ai_play.txt', 'w', encoding=\"utf8\")\n",
        "f.write(shakey)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60IIJGrjgMcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}